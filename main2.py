# -*- coding: utf-8 -*-
"""main2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Da_w4dgG7bQc8stHffWXk1BEbw3w2Fj
"""

# 필요한 라이브러리 설치
!pip install shap xgboost umap-learn statsmodels lime tableone

# 필요한 라이브러리 로드
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import lime
import lime.lime_tabular
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
from umap import UMAP
from sklearn.cross_decomposition import CCA
from scipy.stats import pearsonr, spearmanr, kendalltau
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
import statsmodels.api as sm
import tableone

# 데이터 로드 및 전처리
df = pd.read_csv('/content/drive/MyDrive/heart.csv')
X = df.drop(columns=['target'])
y = df['target']
# X = pd.get_dummies(X, drop_first=True) # Remove or comment out this line to keep original categorical columns
# Get dummies for X_scaled instead of X for tableone
X_dummied = pd.get_dummies(X, drop_first=True)
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_dummied), columns=X_dummied.columns)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 3: Tableone을 이용한 데이터 출력
table = tableone.TableOne(df, categorical=['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal'], # Use original column names from df
                          continuous=['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca'], # Use original column names from df
                          groupby='target')
print(table.tabulate())

# Step 4: t-SNE와 UMAP 시각화
tsne = TSNE(n_components=2, random_state=0).fit_transform(X_scaled)
umap = UMAP(n_components=2, random_state=0).fit_transform(X_scaled)
plt.figure(figsize=(14,6))
plt.subplot(1,2,1)
sns.scatterplot(x=tsne[:,0], y=tsne[:,1], hue=y)
plt.title("t-SNE")
plt.subplot(1,2,2)
sns.scatterplot(x=umap[:,0], y=umap[:,1], hue=y)
plt.title("UMAP")
plt.tight_layout()
plt.show()

# Step 5: CCA + 상관 분석
X_corr = X_scaled.iloc[:, :10]
cca = CCA(n_components=1)
cca.fit(X_corr, y.to_frame())
X_c, y_c = cca.transform(X_corr, y.to_frame())
print("CCA correlation:", np.corrcoef(X_c[:,0], y_c[:,0])[0,1])
print("Pearson:", pearsonr(X_corr.iloc[:,0], y)[0])
print("Spearman:", spearmanr(X_corr.iloc[:,1], y)[0])
print("Kendall:", kendalltau(X_corr.iloc[:,2], y)[0])

# Step 6: 머신러닝 5개 모델 성능 비교
models = {
    'Random Forest': RandomForestClassifier(),
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'SVM': SVC(probability=True),
    'KNN': KNeighborsClassifier(),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')
}
for name, model in models.items():
    print(f"\n==== {name} ====")
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    print(classification_report(y_test, pred))

# Step 7: Odds Ratio, CI, p-value
X_logit = sm.add_constant(X_train)
logit = sm.Logit(y_train, X_logit).fit(disp=0)
print(logit.summary2())

# Step 8: SHAP summary, dependency, force plot
explainer = shap.TreeExplainer(models['Random Forest'])
shap_values = explainer.shap_values(X_train)
shap.summary_plot(shap_values, X_train)

# Step 9: LIME 설명
lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X.columns.tolist(),
    class_names=['No Disease', 'Disease'],
    mode='classification'
)
lime_exp = lime_explainer.explain_instance(
    data_row=X_test.iloc[0],
    predict_fn=models['Random Forest'].predict_proba,
    num_features=10
)
lime_exp.show_in_notebook(show_table=True)

# Step 10: Streamlit 코드
streamlit_code = '''
import streamlit as st
import pandas as pd
import joblib

st.title("심장병 예측기")

uploaded_file = st.file_uploader("CSV 환자 정보 업로드", type="csv")
if uploaded_file:
    df = pd.read_csv(uploaded_file)
    model = joblib.load("rf_model.pkl")
    result = model.predict(df)
    st.write("예측 결과:", result)
'''
print(streamlit_code)